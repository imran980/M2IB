# Install required packages
!pip install requests torch pandas numpy tqdm transformers pytorch-grad-cam Pillow

# Set up environment
import os
import argparse
import requests
import torch
import pandas as pd
import numpy as np
import sys
from tqdm import tqdm
from random import sample
from transformers import CLIPProcessor, CLIPModel, CLIPTokenizerFast
from PIL import Image
from pytorch_grad_cam.metrics.cam_mult_image import DropInConfidence, IncreaseInConfidence

# Add the scripts directory to the Python path
sys.path.append('/content/M2IB')

# Import custom modules
from scripts.utils import ImageFeatureExtractor, TextFeatureExtractor, CosSimilarity
from scripts.methods import vision_heatmap_iba, text_heatmap_iba

# Set environment variables and device
os.environ["TOKENIZERS_PARALLELISM"] = "false"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def get_metrics(image_feat, vmap, text_ids, tmap, model):
    results = {}
    with torch.no_grad():
        # Define target function as the cos similarity of embeddings
        vtargets = [CosSimilarity(model.get_text_features(text_ids).to(device))]
        ttargets = [CosSimilarity(model.get_image_features(image_feat).to(device))]
        # Remove start and end token
        text_ids = text_ids[:,1:-1]
        tmap = np.expand_dims(tmap, axis=0)[:,1:-1]
        # Binarize text attention map
        tmap = tmap > np.percentile(tmap, 50)
        # Use pytorch_grad_cam metrics
        results['vdrop'] = DropInConfidence()(image_feat, vmap, vtargets, ImageFeatureExtractor(model))[0][0]*100
        results['vincr'] = IncreaseInConfidence()(image_feat, vmap, vtargets, ImageFeatureExtractor(model))[0][0]*100
        results['tdrop'] = DropInConfidence()(text_ids, tmap, ttargets, TextFeatureExtractor(model))[0][0]*100
        results['tincr'] = IncreaseInConfidence()(text_ids, tmap, ttargets, TextFeatureExtractor(model))[0][0]*100
        
        # Add ROAR indicator
        results['vroar'] = calculate_roar(image_feat, vmap, text_ids, model, 'vision')
        results['troar'] = calculate_roar(text_ids, tmap, image_feat, model, 'text')
    return results

def calculate_roar(feature, attention_map, target, model, mode):
    # Simplified ROAR implementation
    # Remove top 10% of features based on attention map
    k = int(0.1 * attention_map.size)
    top_indices = np.argpartition(attention_map.flatten(), -k)[-k:]
    
    if mode == 'vision':
        masked_feature = feature.clone()
        masked_feature[:, :, top_indices // attention_map.shape[1], top_indices % attention_map.shape[1]] = 0
        new_similarity = CosSimilarity(model.get_text_features(target).to(device))(masked_feature)
    else:
        masked_ids = text_ids.clone()
        masked_ids[:, top_indices] = model.config.pad_token_id
        new_similarity = CosSimilarity(model.get_image_features(target).to(device))(masked_ids)
    
    # Calculate ROAR score (difference in similarity after removing important features)
    original_similarity = CosSimilarity(model.get_text_features(target).to(device))(feature) if mode == 'vision' else CosSimilarity(model.get_image_features(target).to(device))(text_ids)
    roar_score = (original_similarity - new_similarity) / original_similarity * 100
    
    return roar_score.item()

def main(args):
    print("Loading models ...")
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    tokenizer = CLIPTokenizerFast.from_pretrained("openai/clip-vit-base-patch32")

    df = pd.read_csv(args.data_path, sep='\t', header=None, names=['text', 'image_path'])
    sampled_df = df.sample(n=args.samples)

    all_results = []
    print("Evaluating ...")
    for _, row in tqdm(sampled_df.iterrows()):
        text, image_path = row['text'], row['image_path']

        # Load and preprocess image
        try:
            image = Image.open(requests.get(image_path, stream=True, timeout=5).raw) if 'http' in image_path else Image.open(image_path).convert('RGB')
        except:
            print(f"Unable to load image at {image_path}", flush=True)
            continue

        image_feat = processor(images=image, return_tensors="pt")['pixel_values'].to(device)
        text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)]).to(device)

        vmap = vision_heatmap_iba(text_ids, image_feat, model, args.vlayer, args.vbeta, args.vvar, progbar=False)
        tmap = text_heatmap_iba(text_ids, image_feat, model, args.tlayer, args.tbeta, args.tvar, progbar=False)

        results = get_metrics(image_feat, vmap, text_ids, tmap, model)
        results['image'] = image_path
        results['text'] = text
        all_results.append(results)

    all_results = pd.DataFrame(all_results)
    print("Mean of results:")
    print(all_results.mean(numeric_only=True), flush=True)

    all_results.to_csv(args.output_path, index=False)
    print(f"All results saved to {args.output_path}")

# Set up argument parser for Colab
class Args:
    def __init__(self, data_path, output_path, samples=500, vbeta=0.1, vvar=1, vlayer=9, tbeta=0.1, tvar=1, tlayer=9):
        self.data_path = data_path
        self.output_path = output_path
        self.samples = samples
        self.vbeta = vbeta
        self.vvar = vvar
        self.vlayer = vlayer
        self.tbeta = tbeta
        self.tvar = tvar
        self.tlayer = tlayer

# Set your parameters here
args = Args(
    data_path='/content/Validation_GCC-1.1.0-Validation.tsv',  # Replace with your dataset path
    output_path='/content/results.csv',
    samples=100  # Reduced for quicker execution in Colab
)

# Run the main function
main(args)

# Print a summary of the results
print("Results summary:")
results_df = pd.read_csv(args.output_path)
print(results_df.describe())

# Optional: Display some sample images and their captions
from IPython.display import display, Image as IPImage

for i, row in results_df.head().iterrows():
    print(f"Text: {row['text']}")
    display(IPImage(url=row['image']))
    print("---")
