# Install required packages
!pip install requests torch pandas numpy tqdm transformers pytorch-grad-cam Pillow

# Set up environment
import os
import argparse
import requests
import torch
import pandas as pd
import numpy as np
import sys
from tqdm import tqdm
from random import sample
from transformers import CLIPProcessor, CLIPModel, CLIPTokenizerFast
from PIL import Image
from pytorch_grad_cam.metrics.cam_mult_image import DropInConfidence, IncreaseInConfidence

# Add the scripts directory to the Python path
sys.path.append('/content/M2IB')

# Import custom modules
from scripts.utils import ImageFeatureExtractor, TextFeatureExtractor, CosSimilarity
from scripts.methods import vision_heatmap_iba, text_heatmap_iba

# Set environment variables and device
os.environ["TOKENIZERS_PARALLELISM"] = "false"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def get_metrics(image_feat, vmap, text_ids, tmap, model):
    results = {}
    with torch.no_grad():
        # Define target function as the cos similarity of embeddings
        vtargets = [CosSimilarity(model.get_text_features(text_ids).to(device))]
        ttargets = [CosSimilarity(model.get_image_features(image_feat).to(device))]
        # Remove start and end token
        text_ids = text_ids[:,1:-1]
        tmap = np.expand_dims(tmap, axis=0)[:,1:-1]
        # Binarize text attention map
        tmap = tmap > np.percentile(tmap, 50)
        # Use pytorch_grad_cam metrics
        results['vdrop'] = DropInConfidence()(image_feat, vmap, vtargets, ImageFeatureExtractor(model))[0][0]*100
        results['vincr'] = IncreaseInConfidence()(image_feat, vmap, vtargets, ImageFeatureExtractor(model))[0][0]*100
        results['tdrop'] = DropInConfidence()(text_ids, tmap, ttargets, TextFeatureExtractor(model))[0][0]*100
        results['tincr'] = IncreaseInConfidence()(text_ids, tmap, ttargets, TextFeatureExtractor(model))[0][0]*100
        
        # Add ROAR indicator
        results['vroar'] = calculate_roar(image_feat, vmap, text_ids, model, 'vision')
        results['troar'] = calculate_roar(text_ids, tmap, image_feat, model, 'text')
    return results

def calculate_roar(feature, attention_map, target, model, mode):
    # Simplified ROAR implementation
    # Remove top 10% of features based on attention map
    k = int(0.1 * attention_map.size)
    top_indices = np.argpartition(attention_map.flatten(), -k)[-k:]
    
    if mode == 'vision':
        masked_feature = feature.clone()
        masked_feature[:, :, top_indices // attention_map.shape[1], top_indices % attention_map.shape[1]] = 0
        new_similarity = CosSimilarity(model.get_text_features(target).to(device))(masked_feature)
    else:
        masked_ids = text_ids.clone()
        masked_ids[:, top_indices] = model.config.pad_token_id
        new_similarity = CosSimilarity(model.get_image_features(target).to(device))(masked_ids)
    
    # Calculate ROAR score (difference in similarity after removing important features)
    original_similarity = CosSimilarity(model.get_text_features(target).to(device))(feature) if mode == 'vision' else CosSimilarity(model.get_image_features(target).to(device))(text_ids)
    roar_score = (original_similarity - new_similarity) / original_similarity * 100
    
    return roar_score.item()

def main(args):
    print("Loading models ...")
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    tokenizer = CLIPTokenizerFast.from_pretrained("openai/clip-vit-base-patch32")

    df = pd.read_csv(args.data_path, sep='\t', header=None, names=['text', 'image_path'])
    sampled_df = df.sample(n=args.samples)

    all_results = []
    print("Evaluating ...")
    for _, row in tqdm(sampled_df.iterrows()):
        text, image_path = row['text'], row['image_path']

        # Load and preprocess image
        try:
            image = Image.open(requests.get(image_path, stream=True, timeout=5).raw) if 'http' in image_path else Image.open(image_path).convert('RGB')
        except:
            print(f"Unable to load image at {image_path}", flush=True)
            continue

        image_feat = processor(images=image, return_tensors="pt")['pixel_values'].to(device)
        text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)]).to(device)

        vmap = vision_heatmap_iba(text_ids, image_feat, model, args.vlayer, args.vbeta, args.vvar, progbar=False)
        tmap = text_heatmap_iba(text_ids, image_feat, model, args.tlayer, args.tbeta, args.tvar, progbar=False)

        results = get_metrics(image_feat, vmap, text_ids, tmap, model)
        results['image'] = image_path
        results['text'] = text
        all_results.append(results)

    all_results = pd.DataFrame(all_results)
    print("Mean of results:")
    print(all_results.mean(numeric_only=True), flush=True)

    all_results.to_csv(args.output_path, index=False)
    print(f"All results saved to {args.output_path}")

# Set up argument parser for Colab
class Args:
    def __init__(self, data_path, output_path, samples=500, vbeta=0.1, vvar=1, vlayer=9, tbeta=0.1, tvar=1, tlayer=9):
        self.data_path = data_path
        self.output_path = output_path
        self.samples = samples
        self.vbeta = vbeta
        self.vvar = vvar
        self.vlayer = vlayer
        self.tbeta = tbeta
        self.tvar = tvar
        self.tlayer = tlayer

# Set your parameters here
args = Args(
    data_path='/content/Validation_GCC-1.1.0-Validation.tsv',  # Replace with your dataset path
    output_path='/content/results.csv',
    samples=100  # Reduced for quicker execution in Colab
)

# Run the main function
main(args)

# Print a summary of the results
print("Results summary:")
results_df = pd.read_csv(args.output_path)
print(results_df.describe())

# Optional: Display some sample images and their captions
from IPython.display import display, Image as IPImage

for i, row in results_df.head().iterrows():
    print(f"Text: {row['text']}")
    display(IPImage(url=row['image']))
    print("---")




import os
import pydicom
import random
from PIL import Image
import io
from tqdm import tqdm
import os
import argparse
import requests
import torch
import pandas as pd
import numpy as np
import sys
import clip
from tqdm import tqdm
from random import sample
from transformers import CLIPProcessor, CLIPModel, CLIPTokenizerFast
from PIL import Image
from pytorch_grad_cam.metrics.cam_mult_image import DropInConfidence, IncreaseInConfidence

# Add the scripts directory to the Python path
sys.path.append('/content/M2IB')

# Import custom modules
from scripts.utils import ImageFeatureExtractor, TextFeatureExtractor, CosSimilarity
from scripts.methods import vision_heatmap_iba, text_heatmap_iba

# Set environment variables and device
os.environ["TOKENIZERS_PARALLELISM"] = "false"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def load_dicom(file_path):
    dicom = pydicom.dcmread(file_path)
    image = dicom.pixel_array
    image = Image.fromarray(image).convert('RGB')
    return image

def get_report_text(report_path):
    with open(report_path, 'r') as f:
        report = f.read()
    # Extract the relevant section from the report
    # This is a simplification; you might need more sophisticated text extraction
    findings_start = report.find('FINDINGS:')
    impression_start = report.find('IMPRESSION:')
    if findings_start != -1 and impression_start != -1:
        return report[findings_start:impression_start].strip()
    return report.strip()

def get_metrics(image_feat, vmap, text_ids, tmap, model):
    results = {}
    with torch.no_grad():
        # Define target function as the cos similarity of embeddings
        vtargets = [CosSimilarity(model.get_text_features(text_ids).to(device))]
        ttargets = [CosSimilarity(model.get_image_features(image_feat).to(device))]
        
        # Remove start and end token
        text_ids = text_ids[:, 1:-1]
        tmap = np.expand_dims(tmap, axis=0)[:, 1:-1]
        
        # Binarize text attention map
        tmap = tmap > np.percentile(tmap, 50)
        
        # Use pytorch_grad_cam metrics
        results['vdrop'] = DropInConfidence()(image_feat, vmap, vtargets, ImageFeatureExtractor(model))[0][0] * 100
        results['vincr'] = IncreaseInConfidence()(image_feat, vmap, vtargets, ImageFeatureExtractor(model))[0][0] * 100
        results['tdrop'] = DropInConfidence()(text_ids, tmap, ttargets, TextFeatureExtractor(model))[0][0] * 100
        results['tincr'] = IncreaseInConfidence()(text_ids, tmap, ttargets, TextFeatureExtractor(model))[0][0] * 100
    
    return results

def get_image_text_pairs(base_path, num_samples):
    pairs = []
    print(f"Searching for image-text pairs in {base_path}")
    
    # Iterate through p1* folders
    for p_folder in os.listdir(base_path):
        if p_folder.startswith('p1'):
            p_path = os.path.join(base_path, p_folder)
            print(f"Examining p folder: {p_folder}")
            
            # Iterate through patient folders
            for patient_folder in os.listdir(p_path):
                patient_path = os.path.join(p_path, patient_folder)
                if os.path.isdir(patient_path):
                    print(f"Examining patient folder: {patient_folder}")
                    
                    # Iterate through files in patient folder
                    for file in os.listdir(patient_path):
                        if file.endswith('.txt'):
                            study_id = file[:-4]
                            report_path = os.path.join(patient_path, file)
                            study_folder = os.path.join(patient_path, study_id)
                            
                            if os.path.isdir(study_folder):
                                for image_file in os.listdir(study_folder):
                                    if image_file.endswith('.dcm'):
                                        image_path = os.path.join(study_folder, image_file)
                                        pairs.append((image_path, report_path))
                                        print(f"Found pair: {image_path}, {report_path}")
                                        break  # Only take one image per report for simplicity
    
    print(f"Total pairs found: {len(pairs)}")
    return random.sample(pairs, min(num_samples, len(pairs)))

def main(args):
    print("Loading models ...")
    med_model, _ = clip.load("ViT-B/32", device=device)
    med_model.load_state_dict(torch.load("scripts/clip-imp-pretrained_128_6_after_4.pt", map_location=device))
    model = ClipWrapper(med_model)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    tokenizer = CLIPTokenizerFast.from_pretrained("openai/clip-vit-base-patch32")

    print(f"Getting image-text pairs from {args.data_path}")
    image_text_pairs = get_image_text_pairs(args.data_path, args.samples)
    print(f"Number of pairs to process: {len(image_text_pairs)}")

    all_results = []
    print("Evaluating ...")
    for image_path, report_path in tqdm(image_text_pairs):
        print(f"Processing image: {image_path}")
        # Load and preprocess image
        try:
            image = load_dicom(image_path)
            text = get_report_text(report_path)
        except Exception as e:
            print(f"Error processing {image_path} or {report_path}: {str(e)}")
            continue

        # Get report text
        text = get_report_text(report_path)

        image_feat = processor(images=image, return_tensors="pt")['pixel_values'].to(device)
        max_tokens = 77  # or whatever your model's limit is
        text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True, max_length=max_tokens, truncation=True)]).to(device)

        vmap = vision_heatmap_iba(text_ids, image_feat, model, args.vlayer, args.vbeta, args.vvar, progbar=False)
        tmap = text_heatmap_iba(text_ids, image_feat, model, args.tlayer, args.tbeta, args.tvar, progbar=False)

        results = get_metrics(image_feat, vmap, text_ids, tmap, model)
        results['image'] = image_path
        results['text'] = text
        all_results.append(results)

    if all_results:
        all_results = pd.DataFrame(all_results)
        print("Mean of results:")
        print(all_results.mean(numeric_only=True), flush=True)

        all_results.to_csv(args.output_path, index=False)
        print(f"All results saved to {args.output_path}")
    else:
        print("No results were generated. Check if any image-text pairs were processed.")

# Modify the Args class to include the new data path
class Args:
    def __init__(self, data_path, output_path, samples=20, vbeta=0.13, vvar=1, vlayer=9, tbeta=0.13, tvar=1, tlayer=9):
        self.data_path = data_path
        self.output_path = output_path
        self.samples = samples
        self.vbeta = vbeta
        self.vvar = vvar
        self.vlayer = vlayer
        self.tbeta = tbeta
        self.tvar = tvar
        self.tlayer = tlayer

# Set your parameters here
args = Args(
    data_path=r'D:\MS-CXR\files',  # Path to the 'files' folder containing p10, p11, etc.
    output_path=r'D:\MS-CXR\results.csv',
    samples=3  # Adjust as needed
)

# Run the main function
main(args)



#=========================================================================================================================
import os
import random
import pydicom
from PIL import Image
import torch
from transformers import CLIPModel, CLIPProcessor, CLIPTokenizerFast
import pandas as pd
from tqdm import tqdm

def load_dicom(file_path):
    dicom = pydicom.dcmread(file_path)
    image = dicom.pixel_array
    image = Image.fromarray(image).convert('RGB')
    return image

def get_report_text(report_path):
    with open(report_path, 'r') as f:
        report = f.read()
    # Extract the relevant section from the report
    findings_start = report.find('FINDINGS:')
    impression_start = report.find('IMPRESSION:')
    if findings_start != -1 and impression_start != -1:
        return report[findings_start:impression_start].strip()
    else:
        print(f"Warning: 'FINDINGS' section not found in file: {os.path.basename(report_path)}")
        print(f"Full path: {report_path}")
        return report.strip()

def get_image_text_pairs(base_path, num_samples):
    pairs = []
    files_path = os.path.join(base_path, 'files')
    for patient_folder in os.listdir(files_path):
        patient_path = os.path.join(files_path, patient_folder)
        if os.path.isdir(patient_path):
            for study_folder in os.listdir(patient_path):
                study_path = os.path.join(patient_path, study_folder)
                if os.path.isdir(study_path):
                    # Find the report file
                    report_file = f"{study_folder}.txt"
                    report_path = os.path.join(patient_path, report_file)
                    if os.path.isfile(report_path):
                        # Get image files
                        for image_file in os.listdir(study_path):
                            if image_file.endswith('.dcm'):
                                image_path = os.path.join(study_path, image_file)
                                pairs.append((image_path, report_path))
                                # Only take one image per report for simplicity
                                break  
    return random.sample(pairs, min(num_samples, len(pairs)))

def main(args):
    print("Loading models ...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    tokenizer = CLIPTokenizerFast.from_pretrained("openai/clip-vit-base-patch32")

    image_text_pairs = get_image_text_pairs(args.data_path, args.samples)

    all_results = []
    print("Evaluating ...")
    for image_path, report_path in tqdm(image_text_pairs):
        try:
            image = load_dicom(image_path)
        except Exception as e:
            print(f"Unable to load image at {image_path}: {e}", flush=True)
            continue

        text = get_report_text(report_path)
        image_feat = processor(images=image, return_tensors="pt")['pixel_values'].to(device)
        text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)]).to(device)

        with torch.no_grad():
            vmap = vision_heatmap_iba(text_ids, image_feat, model, args.vlayer, args.vbeta, args.vvar, progbar=False)
            tmap = text_heatmap_iba(text_ids, image_feat, model, args.tlayer, args.tbeta, args.tvar, progbar=False)

        results = get_metrics(image_feat, vmap, text_ids, tmap, model)
        results['image'] = image_path
        results['text'] = text
        all_results.append(results)

    all_results_df = pd.DataFrame(all_results)
    print("Mean of results:")
    print(all_results_df.mean(numeric_only=True), flush=True)

    all_results_df.to_csv(args.output_path, index=False)
    print(f"All results saved to {args.output_path}")

class Args:
    def __init__(self, data_path, output_path, samples=20, vbeta=0.13, vvar=1, vlayer=9, tbeta=0.13, tvar=1, tlayer=9):
        self.data_path = data_path
        self.output_path = output_path
        self.samples = samples
        self.vbeta = vbeta
        self.vvar = vvar
        self.vlayer = vlayer
        self.tbeta = tbeta
        self.tvar = tvar
        self.tlayer = tlayer

args = Args(
    data_path='/path/to/ms-cxr1/',  # Replace with the path to your MS-CXR dataset
    output_path='/content/results.csv',
    samples=20
)

main(args)


#--------------------------------------------------------------------------------------------------------------------
import os
import random
import pydicom
from PIL import Image
import torch
from transformers import CLIPModel, CLIPProcessor, CLIPTokenizerFast
import pandas as pd
from tqdm import tqdm
import numpy as np

def load_dicom(file_path):
    dicom = pydicom.dcmread(file_path)
    image = dicom.pixel_array
    image = Image.fromarray(image).convert('RGB')
    return image

def get_report_text(report_path):
    with open(report_path, 'r') as f:
        report = f.read()
    # Extract the relevant section from the report
    findings_start = report.find('FINDINGS:')
    impression_start = report.find('IMPRESSION:')
    if findings_start != -1 and impression_start != -1:
        return report[findings_start:impression_start].strip()
    else:
        print(f"Warning: 'FINDINGS' section not found in file: {os.path.basename(report_path)}")
        print(f"Full path: {report_path}")
        return report.strip()

def get_image_text_pairs(base_path, num_samples):
    pairs = []
    files_path = os.path.join(base_path, 'files')
    for patient_folder in os.listdir(files_path):
        patient_path = os.path.join(files_path, patient_folder)
        if os.path.isdir(patient_path):
            for study_folder in os.listdir(patient_path):
                study_path = os.path.join(patient_path, study_folder)
                if os.path.isdir(study_path):
                    # Find the report file
                    report_file = f"{study_folder}.txt"
                    report_path = os.path.join(patient_path, report_file)
                    if os.path.isfile(report_path):
                        # Get image files
                        for image_file in os.listdir(study_path):
                            if image_file.endswith('.dcm'):
                                image_path = os.path.join(study_path, image_file)
                                pairs.append((image_path, report_path))
                                # Only take one image per report for simplicity
                                break  
    return random.sample(pairs, min(num_samples, len(pairs)))
    
    #------------------------------------New code--------------------------------------------------------------------------
    #----------------------------------------------------------------------------------------------------------------------
import os
import random
import pydicom
from PIL import Image
import torch
from transformers import CLIPModel, CLIPProcessor, CLIPTokenizerFast
import pandas as pd
from tqdm import tqdm
import numpy as np

def load_dicom(file_path):
    dicom = pydicom.dcmread(file_path)
    image = dicom.pixel_array
    image = Image.fromarray(image).convert('RGB')
    return image

def get_report_text(report_path):
    with open(report_path, 'r') as f:
        report = f.read()
    # Extract the relevant section from the report
    findings_start = report.find('FINDINGS:')
    impression_start = report.find('IMPRESSION:')
    if findings_start != -1 and impression_start != -1:
        return report[findings_start:impression_start].strip()
    else:
        print(f"Warning: 'FINDINGS' section not found in file: {os.path.basename(report_path)}")
        print(f"Full path: {report_path}")
        return report.strip()

def get_image_text_pairs(base_path, num_samples):
    pairs = []
    files_path = os.path.join(base_path, 'files')
    for patient_folder in os.listdir(files_path):
        patient_path = os.path.join(files_path, patient_folder)
        if os.path.isdir(patient_path):
            for study_folder in os.listdir(patient_path):
                study_path = os.path.join(patient_path, study_folder)
                if os.path.isdir(study_path):
                    # Find the report file
                    report_file = f"{study_folder}.txt"
                    report_path = os.path.join(patient_path, report_file)
                    if os.path.isfile(report_path):
                        # Get image files
                        for image_file in os.listdir(study_path):
                            if image_file.endswith('.dcm'):
                                image_path = os.path.join(study_path, image_file)
                                pairs.append((image_path, report_path))
                                # Only take one image per report for simplicity
                                break  
    return random.sample(pairs, min(num_samples, len(pairs)))
    
    
    #----------------------------------------------------------------------------------------------------------------------
    class CustomDataset(Dataset):
    def __init__(self, image_text_pairs, processor, tokenizer, degraded=False):
        self.pairs = image_text_pairs
        self.processor = processor
        self.tokenizer = tokenizer
        self.degraded = degraded

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        image_path, report_path = self.pairs[idx]
        image = load_dicom(image_path)
        text = get_report_text(report_path)

        if self.degraded:
            image = degrade_image(image, vmap)
            text = degrade_text(text, tmap)

        image_feat = self.processor(images=image, return_tensors="pt")['pixel_values'].squeeze(0)
        text_ids = self.tokenizer.encode(text, add_special_tokens=True, return_tensors="pt").squeeze(0)

        return image_feat, text_ids

def degrade_image(image, vmap):
    # Replace important parts with channel means
    degraded = image.copy()
    for c in range(3):  # For each channel
        channel_mean = np.mean(image[:,:,c])
        mask = vmap > np.percentile(vmap, 90)  # Top 10% important pixels
        degraded[:,:,c][mask] = channel_mean
    return degraded

def degrade_text(text, tmap):
    # Replace important tokens with padding token
    tokens = text.split()
    important_indices = np.argsort(tmap)[-int(len(tokens)*0.1):]  # Top 10% important tokens
    for idx in important_indices:
        tokens[idx] = "[PAD]"
    return " ".join(tokens)

def fine_tune_clip(model, train_loader, val_loader, device, num_epochs=3):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
    best_loss = float('inf')

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for image_features, text_ids in train_loader:
            image_features, text_ids = image_features.to(device), text_ids.to(device)
            outputs = model(input_ids=text_ids, pixel_values=image_features)
            loss = outputs.loss
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}")

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for image_features, text_ids in val_loader:
                image_features, text_ids = image_features.to(device), text_ids.to(device)
                outputs = model(input_ids=text_ids, pixel_values=image_features)
                val_loss += outputs.loss.item()

        avg_val_loss = val_loss / len(val_loader)
        print(f"Validation Loss: {avg_val_loss:.4f}")

        if avg_val_loss < best_loss:
            best_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_model.pth')

    return best_loss

def calculate_roar_plus(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    tokenizer = CLIPTokenizerFast.from_pretrained("openai/clip-vit-base-patch32")

    image_text_pairs = get_image_text_pairs(args.data_path, args.samples)
    train_pairs, val_pairs = train_test_split(image_text_pairs, test_size=0.2, random_state=42)

    # Original data
    train_dataset = CustomDataset(train_pairs, processor, tokenizer)
    val_dataset = CustomDataset(val_pairs, processor, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)

    original_loss = fine_tune_clip(model, train_loader, val_loader, device)

    # Degraded data
    degraded_train_dataset = CustomDataset(train_pairs, processor, tokenizer, degraded=True)
    degraded_train_loader = DataLoader(degraded_train_dataset, batch_size=32, shuffle=True)

    corrupted_loss = fine_tune_clip(model, degraded_train_loader, val_loader, device)

    roar_plus_score = (corrupted_loss - original_loss) / original_loss
    return roar_plus_score
    #----------------------------------------------------------------------------------------------------------------------

def main(args):
    print("Loading models ...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    tokenizer = CLIPTokenizerFast.from_pretrained("openai/clip-vit-base-patch32")

    image_text_pairs = get_image_text_pairs(args.data_path, args.samples)
    #----------------------------------------------------------------------------------------------------------------------
        # Calculate ROAR+ score
    roar_plus_scores = []
    for _ in range(5):  # Repeat 5 times
        score = calculate_roar_plus(args)
        roar_plus_scores.append(score)
    
    avg_roar_plus_score = np.mean(roar_plus_scores)
    print(f"Average ROAR+ Score: {avg_roar_plus_score:.4f}")
    #----------------------------------------------------------------------------------------------------------------------
    all_results = []
    print("Evaluating ...")
    for image_path, report_path in tqdm(image_text_pairs):
        try:
            image = load_dicom(image_path)
        except Exception as e:
            print(f"Unable to load image at {image_path}: {e}", flush=True)
            continue

        text = get_report_text(report_path)
        image_feat = processor(images=image, return_tensors="pt")['pixel_values'].to(device)
        text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)]).to(device)

        with torch.no_grad():
            vmap = vision_heatmap_iba(text_ids, image_feat, model, args.vlayer, args.vbeta, args.vvar, progbar=False)
            tmap = text_heatmap_iba(text_ids, image_feat, model, args.tlayer, args.tbeta, args.tvar, progbar=False)

        results = get_metrics(image_feat, vmap, text_ids, tmap, model)
        results['image'] = image_path
        results['text'] = text
        all_results.append(results)

    all_results_df = pd.DataFrame(all_results)
    print("Mean of results:")
    print(all_results_df.mean(numeric_only=True), flush=True)

    all_results_df.to_csv(args.output_path, index=False)
    print(f"All results saved to {args.output_path}")

class Args:
    def __init__(self, data_path, output_path, samples=20, vbeta=0.13, vvar=1, vlayer=9, tbeta=0.13, tvar=1, tlayer=9):
        self.data_path = data_path
        self.output_path = output_path
        self.samples = samples
        self.vbeta = vbeta
        self.vvar = vvar
        self.vlayer = vlayer
        self.tbeta = tbeta
        self.tvar = tvar
        self.tlayer = tlayer

args = Args(
    data_path='/path/to/ms-cxr1/',  # Replace with the path to your MS-CXR dataset
    output_path='/content/results.csv',
    samples=20
)

main(args)
